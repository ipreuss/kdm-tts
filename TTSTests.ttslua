-- TTS Console Test Harness
-- These tests run inside TTS via console commands to verify actual behavior
-- Usage: Type ">testhelp" in TTS chat to see available tests
--
-- Test modules are organized by domain in TTSTests/ directory:
--   StrainTests.ttslua      - Fighting Arts, Vermin, Card State fallback
--   AtmosphericTests.ttslua - Atmospheric Change milestone
--   DialogTests.ttslua      - DialogFromSpec callback timing
--   TrashTests.ttslua       - Trash restore functionality
--   MilestoneTests.ttslua   - Milestone execution (AddCard + SpawnForSurvivor)
--   ConsequenceTests.ttslua - ConsequenceApplicator operations
--   PatternTests.ttslua     - Pattern deck spawning, Seed Patterns shuffle
--   BattleTests.ttslua      - Weapon pairing (Aya's weapons, Bone Hatchet)
--   ResourceRewardsTests.ttslua - Resource rewards button and spawning
--   TestRegistry.ttslua     - Test registry with bead tags
--   TestErrorCapture.ttslua - Error capture for test framework

local Console = require("Kdm/Console")
local Log = require("Kdm/Log")
local log = Log.ForModule("TTSTests")

-- Import test modules
local StrainTests = require("Kdm/TTSTests/StrainTests")
local AtmosphericTests = require("Kdm/TTSTests/AtmosphericTests")
local DialogTests = require("Kdm/TTSTests/DialogTests")
local TrashTests = require("Kdm/TTSTests/TrashTests")
local MilestoneTests = require("Kdm/TTSTests/MilestoneTests")
local ConsequenceTests = require("Kdm/TTSTests/ConsequenceTests")
local PatternTests = require("Kdm/TTSTests/PatternTests")
local BattleTests = require("Kdm/TTSTests/BattleTests")
local ResourceRewardsTests = require("Kdm/TTSTests/ResourceRewardsTests")
local HuntTests = require("Kdm/TTSTests/HuntTests")

-- Import test infrastructure modules
local TestRegistry = require("Kdm/TTSTests/TestRegistry")
local TestErrorCapture = require("Kdm/TTSTests/TestErrorCapture")

---------------------------------------------------------------------------------------------------

local TTSTests = {}

---------------------------------------------------------------------------------------------------

function TTSTests.Init()
    Console.AddCommand("testhelp", function(args)
        log:Printf("TTS Test Commands:")
        log:Printf("")
        log:Printf("  Discovery:")
        log:Printf("    >testlist              - List all tests alphabetically")
        log:Printf("    >testsuite list        - List beads with test counts and titles")
        log:Printf("    >testsuite domains     - List domains with test counts")
        log:Printf("")
        log:Printf("  Selective Execution:")
        log:Printf("    >testrun <name>        - Run single test by exact name")
        log:Printf("    >testsuite <bead>      - Run all tests for a bead")
        log:Printf("    >testsuite domain <n>  - Run all tests for a domain")
        log:Printf("    >testcurrent           - Run tests for FOCUS_BEAD (%s)", TestRegistry.FOCUS_BEAD)
        log:Printf("    >testpriority          - Run FOCUS_BEAD first, then others if pass")
        log:Printf("")
        log:Printf("  Full Suite:")
        log:Printf("    >testall            - Run all TTS tests")
        log:Printf("    >testerrordetect    - Verify error detection (should FAIL)")
        log:Printf("")
        log:Printf("  Individual Tests:")
        log:Printf("    >teststrain [card]  - Test adding a strain fighting art")
        log:Printf("    >teststrainvermin   - Test adding a strain vermin card")
        log:Printf("    >testdialogspec     - Test DialogFromSpec callback timing")
        log:Printf("    >testcardstate [id] - Test archive fallback for stateful names")
        log:Printf("    >testatmospheric    - Test Atmospheric Change milestone")
        log:Printf("    >testheatwaverestore- Test restoring Heat Wave from Trash")
        log:Printf("    >testmilestone      - Test full milestone execution")
        log:Printf("    >testplottwist      - Test Plot Twist milestone")
        log:Printf("    >testconsequence    - Test ConsequenceApplicator operations")
        log:Printf("    >testpatterns       - Test spawning Pattern deck")
        log:Printf("    >testpatterngear    - Test spawning Pattern Gear deck")
        log:Printf("    >testshuffle        - Test Seed Patterns deck shuffle")
        log:Printf("    >testayaspairing    - Test Aya's weapon pairing")
        log:Printf("    >testayasseparate   - Test Aya's weapons separate lines")
        log:Printf("    >testbonehatchet    - Test Bone Hatchet pairing")
        log:Printf("    >testrewardsbutton  - Test rewards button visibility")
        log:Printf("    >testrewardshidden  - Test rewards button hidden")
        log:Printf("    >testrewardsspawn   - Test correct resources spawned")
        log:Printf("    >testrewardsstrange - Test strange resources spawn")
        log:Printf("    >testhuntreveal     - Test hunt party drop triggers reveal")
        log:Printf("    >testhuntflip       - Test face-down cards flip on reveal")
        log:Printf("    >testhuntempty      - Test empty space handling")
        log:Printf("    >testhuntcleanup    - Test cleanup clears revealed cards")
        log:Printf("    >testhuntvisited    - Test visited position tracking")
        log:Printf("    >testhunt2cards     - Test two card types at same location")
        log:Printf("    >testhuntdeck       - Test stacked cards (deck) at location")
        log:Printf("    >testhuntdeckorder  - Test top card revealed first from deck")
        log:Printf("    >testhuntindividualorder - Test top card revealed first (individual)")
        log:Printf("    >testhuntbutton     - Test Next Card button position")
    end, "Show available TTS test commands")

    Console.AddCommand("testall", function(args)
        TTSTests.RunAllTests()
    end, "Run all TTS tests")

    Console.AddCommand("testrun", function(args)
        if #args < 2 then
            log:Printf("Usage: testrun <test_name>")
            log:Printf("Example: testrun Spidicules Button Visible")
            return
        end
        -- Join all args after command name (test names have spaces)
        local testName = table.concat(args, " ", 2)
        TTSTests.RunSingleTest(testName)
    end, "Run a single test by name")

    Console.AddCommand("testsuite", function(args)
        if #args < 2 then
            log:Printf("Usage:")
            log:Printf("  testsuite list           - List beads with test counts")
            log:Printf("  testsuite <bead_id>      - Run tests for a bead")
            log:Printf("  testsuite domains        - List domains with test counts")
            log:Printf("  testsuite domain <name>  - Run tests for a domain")
            return
        end
        local arg = args[2]
        if arg == "list" then
            TestRegistry.PrintBeadSuites()
        elseif arg == "domains" then
            TestRegistry.PrintDomainSuites()
        elseif arg == "domain" then
            if #args < 3 then
                log:Printf("Usage: testsuite domain <name>")
                log:Printf("")
                TestRegistry.PrintDomainSuites()
                return
            end
            TTSTests.RunDomainSuite(args[3])
        else
            TTSTests.RunBeadSuite(arg)
        end
    end, "Run tests by bead or domain")

    Console.AddCommand("testcurrent", function(args)
        TTSTests.RunCurrentBeadTests()
    end, "Run tests for current bead (FOCUS_BEAD)")

    Console.AddCommand("testpriority", function(args)
        TTSTests.RunPriorityTests()
    end, "Run current bead tests first, then others if all pass")

    Console.AddCommand("testerrordetect", function(args)
        TTSTests.VerifyErrorDetection()
    end, "Verify error log detection works (should FAIL)")

    Console.AddCommand("testlist", function(args)
        TestRegistry.PrintAllTests()
    end, "List all available tests alphabetically")

    -- Register all test module commands
    StrainTests.Register()
    AtmosphericTests.Register()
    DialogTests.Register()
    TrashTests.Register()
    MilestoneTests.Register()
    ConsequenceTests.Register()
    PatternTests.Register()
    BattleTests.Register()
    ResourceRewardsTests.Register()
    HuntTests.Register()
end

---------------------------------------------------------------------------------------------------
-- Run All Tests
---------------------------------------------------------------------------------------------------

function TTSTests.RunAllTests()
    log:Printf("=== RUNNING ALL TTS TESTS ===")
    TTSTests.RunTests(TestRegistry.ALL_TESTS)
end

---------------------------------------------------------------------------------------------------
-- Run Single Test by Name
---------------------------------------------------------------------------------------------------

function TTSTests.RunSingleTest(testName)
    local foundTest = nil
    local lowerTestName = string.lower(testName)
    for _, test in ipairs(TestRegistry.ALL_TESTS) do
        if string.lower(test.name) == lowerTestName then
            foundTest = test
            break
        end
    end

    if not foundTest then
        log:Errorf("Test not found: '%s'", testName)
        log:Printf("Use >testhelp to see available test commands")
        return
    end

    log:Printf("=== RUNNING SINGLE TEST: %s ===", testName)
    TTSTests.RunTests({ foundTest })
end

---------------------------------------------------------------------------------------------------
-- Run All Tests for a Bead
---------------------------------------------------------------------------------------------------

function TTSTests.RunBeadSuite(beadId)
    local beadTests = {}
    for _, test in ipairs(TestRegistry.ALL_TESTS) do
        if TestRegistry.TestMatchesBead(test, beadId) then
            table.insert(beadTests, test)
        end
    end

    if #beadTests == 0 then
        log:Errorf("No tests found for bead: '%s'", beadId)
        log:Printf("Check that tests are tagged with bead = \"%s\"", beadId)
        return
    end

    log:Printf("=== RUNNING %d TESTS FOR BEAD %s ===", #beadTests, beadId)
    TTSTests.RunTests(beadTests)
end

---------------------------------------------------------------------------------------------------
-- Run All Tests for a Domain
---------------------------------------------------------------------------------------------------

function TTSTests.RunDomainSuite(domainName)
    local domainTests = TestRegistry.DiscoverTestsByDomain(domainName)

    if #domainTests == 0 then
        log:Errorf("No tests found for domain: '%s'", domainName)
        log:Printf("Available domains:")
        TestRegistry.PrintDomainSuites()
        return
    end

    log:Printf("=== RUNNING %d TESTS FOR DOMAIN %s ===", #domainTests, domainName)
    TTSTests.RunTests(domainTests)
end

---------------------------------------------------------------------------------------------------
-- Run Tests for Current Bead Only
---------------------------------------------------------------------------------------------------

function TTSTests.RunCurrentBeadTests()
    log:Printf("=== RUNNING TESTS FOR CURRENT BEAD: %s ===", TestRegistry.FOCUS_BEAD)
    TTSTests.RunBeadSuite(TestRegistry.FOCUS_BEAD)
end

---------------------------------------------------------------------------------------------------
-- Run Priority Tests (current bead first, then regression if all pass)
---------------------------------------------------------------------------------------------------

function TTSTests.RunPriorityTests()
    log:Printf("=== RUNNING PRIORITY TESTS FOR %s ===", TestRegistry.FOCUS_BEAD)

    local focusedTests = {}
    local regressionTests = {}

    for _, test in ipairs(TestRegistry.ALL_TESTS) do
        if TestRegistry.TestMatchesBead(test, TestRegistry.FOCUS_BEAD) then
            table.insert(focusedTests, test)
        else
            table.insert(regressionTests, test)
        end
    end

    if #focusedTests == 0 then
        log:Printf("No tests tagged with bead '%s'", TestRegistry.FOCUS_BEAD)
        log:Printf("Update FOCUS_BEAD in TTSTests/TestRegistry.ttslua or tag tests with this bead")
        return
    end

    log:Printf("Phase 1: %d focused tests for %s", #focusedTests, TestRegistry.FOCUS_BEAD)
    log:Printf("Phase 2: %d regression tests (if Phase 1 passes)", #regressionTests)

    TTSTests.RunTests(focusedTests, function(passed, failed)
        if failed > 0 then
            log:Errorf("=== STOPPING: %d focused test(s) failed ===", failed)
            log:Printf("Fix focused tests before running regression tests")
            return
        end

        if #regressionTests == 0 then
            log:Printf("=== ALL TESTS COMPLETE (no regression tests) ===")
            return
        end

        log:Printf("")
        log:Printf("=== PHASE 2: REGRESSION TESTS ===")
        log:Printf("All %d focused tests passed, running %d regression tests...", passed, #regressionTests)

        Wait.frames(function()
            TTSTests.RunTests(regressionTests)
        end, 30)
    end)
end

---------------------------------------------------------------------------------------------------
-- Generic Test Runner
-- Optional onAllComplete callback receives (passedCount, failedCount)
---------------------------------------------------------------------------------------------------

function TTSTests.RunTests(tests, onAllComplete)

    local currentTest = 1
    local totalTests = #tests
    local passedTests = 0
    local failedTests = 0
    local failedNames = {}

    local function runNextTest()
        if currentTest > totalTests then
            log:Printf("=== TEST BATCH COMPLETE ===")
            log:Printf("Ran %d tests: %d passed, %d failed", totalTests, passedTests, failedTests)
            if failedTests > 0 then
                log:Errorf("Failed tests:")
                for _, name in ipairs(failedNames) do
                    log:Errorf("  - %s", name)
                end
            end
            if onAllComplete then
                onAllComplete(passedTests, failedTests)
            end
            return
        end

        local test = tests[currentTest]
        log:Printf("=== TEST %d/%d: %s ===", currentTest, totalTests, test.name)

        -- Start capturing errors before test execution
        TestErrorCapture.Start()

        local completed = false
        local function onTestComplete(success)
            if completed then return end  -- Prevent double-completion
            completed = true

            -- Stop capturing and check for errors
            local errors = TestErrorCapture.Stop()
            if #errors > 0 then
                log:Printf("TEST FAILED: %d error(s) logged during execution:", #errors)
                for _, err in ipairs(errors) do
                    log:Printf("  %s", err.message)
                end
                success = false  -- Force failure even if assertions passed
            end

            if success == false then
                failedTests = failedTests + 1
                table.insert(failedNames, test.name)
            else
                passedTests = passedTests + 1
            end

            currentTest = currentTest + 1

            -- Wait for cleanup before starting next test
            Wait.frames(function()
                runNextTest()
            end, 30)
        end

        local ok, err = pcall(function()
            test.fn(onTestComplete)
        end)
        if not ok then
            -- Stop capturing before logging the pcall error
            TestErrorCapture.Stop()
            log:Errorf("TEST FAILED: %s - %s", test.name, tostring(err))
            -- Wait for async operations to settle before marking complete
            Wait.frames(function()
                onTestComplete(false)
            end, 60)
        end
    end

    runNextTest()
end

---------------------------------------------------------------------------------------------------
-- Verify Error Detection (kdm-fl2.2)
-- This test should FAIL to prove error log detection works
---------------------------------------------------------------------------------------------------

function TTSTests.VerifyErrorDetection()
    log:Printf("=== VERIFYING ERROR DETECTION (should FAIL) ===")
    log:Printf("This test intentionally logs an error to verify the detection mechanism.")
    log:Printf("Expected result: TEST FAILED with error message shown.")
    log:Printf("")

    local verificationTest = {
        { name = "Error Log Causes Test Failure", fn = function(onComplete)
            -- This test verifies that error logs cause test failure even when assertions pass
            log:Printf("Logging a warning (should not cause failure)")
            log:Errorf("Intentional error for verification - this should cause test failure")
            -- Call onComplete with success=true, but the error capture should override to false
            onComplete(true)
        end }
    }

    TTSTests.RunTests(verificationTest, function(passed, failed)
        if failed == 1 then
            log:Printf("")
            log:Printf("[66ff66]SUCCESS: Error detection is working correctly!")
            log:Printf("The test failed as expected because an error was logged.")
        else
            log:Errorf("")
            log:Errorf("ERROR: Error detection is NOT working!")
            log:Errorf("The test should have failed but it passed.")
        end
    end)
end

---------------------------------------------------------------------------------------------------

return TTSTests
